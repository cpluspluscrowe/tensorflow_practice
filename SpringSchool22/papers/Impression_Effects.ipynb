{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-08T23:37:37.016434Z",
     "start_time": "2021-08-08T23:37:32.957686Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.DaliSpark\n",
    "import collection.JavaConverters._\n",
    "import spark.implicits._\n",
    "import collection.mutable._\n",
    "import scala.util.control._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "//val filteredByObjective = data.filter($\"objectivetype\" === \"BRAND_AWARENESS\" || $\"objectivetype\" === \"WEBSITE_VISIT\").repartition(800, $\"accounturn\")\n",
    "case class AccountMemberMostRecentEngagementDate(accounturn: String, memberid: Double, mostRecentEngagementDate: String)\n",
    "//val advertiser = data.select(\"accounturn\").limit(1).collectAsList.asScala(0)\n",
    "//case class AccountMember(accounturn: String, memberid: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-01T02:32:25.781819Z",
     "start_time": "2021-08-01T02:28:36.635201Z"
    }
   },
   "outputs": [],
   "source": [
    "//val accountMembers = filteredByObjective.groupBy($\"accounturn\", $\"memberid\").agg(first(\"memberid\").alias(\"first\")).drop($\"first\").as[AccountMember].collectAsList().asScala//.foreach(t => storeAccountMemberData(filteredByObjective, t.accounturn, t.memberid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-01T20:39:50.875732Z",
     "start_time": "2021-08-01T20:39:48.841823Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-01T18:05:31.417104Z",
     "start_time": "2021-08-01T18:05:29.386781Z"
    }
   },
   "outputs": [],
   "source": [
    "//DaliSpark.writeDataFrame(accounts.toDF(), \"/jobs/tscprep/test/Chad/accounts\", scala.collection.immutable.Map(DaliSpark.OVERWRITE_MODE -> \"ALL\", DaliSpark.OUTPUT_PARALLELISM -> \"1000\"))\n",
    "//val accounts = DaliSpark.createDataFrame(\"/jobs/tscprep/test/Chad/accounts/*\").as[String].collectAsList().asScala\n",
    "//val accounts = Seq(\"urn:li:sponsoredAccount:508014636\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-01T21:06:55.088229Z",
     "start_time": "2021-08-01T20:40:01.556835Z"
    }
   },
   "outputs": [],
   "source": [
    "/*val metrics = Seq(\n",
    "\"landingpageclicks\"\n",
    "\"totalengagements\",\n",
    "\"otherengagements\",\n",
    "\"likes\",\n",
    "\"commentlikes\",\n",
    "\"comments\",\n",
    "\"shares\",\n",
    "\"follows\"\n",
    ")\n",
    "\n",
    "def getMostRecentEngagementDateForMetric(metric: String): Dataset[AccountMemberMostRecentEngagementDate] = {\n",
    "    val engagements = filteredByObjective.filter(col(metric) > 0).repartition(200)\n",
    "    val withMostRecentDateOfEngagement = engagements.groupBy($\"accounturn\", $\"memberid\").\n",
    "    agg(max($\"datepartition\").alias(\"mostRecentEngagementDate\")).\n",
    "    as[AccountMemberMostRecentEngagementDate]\n",
    "    withMostRecentDateOfEngagement\n",
    "}\n",
    "//val loop = new Breaks;\n",
    "\n",
    "//loop.breakable {\n",
    "for(metric <- metrics){\n",
    "    val mostRecentEngagementDateForMetric = getMostRecentEngagementDateForMetric(metric)\n",
    "    val writePath = \"/jobs/tscprep/test/Chad/Metrics/\" + metric\n",
    "    DaliSpark.writeDataFrame(mostRecentEngagementDateForMetric.toDF(), writePath, scala.collection.immutable.Map(DaliSpark.OVERWRITE_MODE -> \"ALL\", DaliSpark.OUTPUT_PARALLELISM -> \"10\"))    \n",
    "//    loop.break;\n",
    "}\n",
    "//}\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution_time": {
     "end_time": "2021-07-31T00:47:54.524072Z",
     "start_time": "2021-07-31T00:46:34.105496Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-03T01:07:57.537655Z",
     "start_time": "2021-08-03T01:07:47.464677Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import spark.implicits._\n",
    "\n",
    "/*case class EmptyRow(accounturn: String, memberid: String, objectivetype: String, summation: Double, D0_1: Integer, D2_3: Integer, D4_7: Integer, D8_30: Integer, \n",
    "                                   D31_60: Integer, D61_90: Integer, D91_120: Integer,\n",
    "                                   D121_150: Integer, D151_180: Integer, D181_210: Integer, \n",
    "                                   D211_240: Integer, D241_270: Integer, D271_300: Integer,\n",
    "                                   D301_330: Integer, D331_360: Integer)\n",
    "*/\n",
    "\n",
    "//def storeAccountMemberData(filteredByObjective: DataFrame, account: String, memberNumber: Double): Unit = {\n",
    "\n",
    "//        val accountId = account.replace(\"urn:li:sponsoredAccount:\",\"\")\n",
    "//        val filterToAccount = filteredByObjective.filter($\"accounturn\" === account)\n",
    "\n",
    "//val metric = \"landingpageclicks\"\n",
    "val metrics = Seq(\n",
    "\"landingpageclicks\",\n",
    "\"totalengagements\",\n",
    "\"otherengagements\",\n",
    "\"likes\",\n",
    "\"commentlikes\",\n",
    "\"comments\",\n",
    "\"shares\",\n",
    "\"follows\"\n",
    ")\n",
    "val objectives = Seq(\n",
    "    \"BRAND_AWARENESS\",\n",
    "    \"WEBSITE_VISIT\"\n",
    ")\n",
    "\n",
    "val months = Seq(\n",
    "Seq(\"2021-07-01-00\", \"2021-08-01-00\"),\n",
    "Seq(\"2021-06-01-00\", \"2021-07-01-00\"),\n",
    "Seq(\"2021-05-01-00\", \"2021-06-01-00\"),\n",
    "Seq(\"2021-04-01-00\", \"2021-05-01-00\"),\n",
    "Seq(\"2021-03-01-00\", \"2021-04-01-00\"),    \n",
    "Seq(\"2021-02-01-00\", \"2021-03-01-00\"),\n",
    "Seq(\"2021-01-01-00\", \"2021-02-01-00\"),\n",
    "Seq(\"2020-12-01-00\", \"2021-01-01-00\"),\n",
    "Seq(\"2020-11-01-00\", \"2020-12-01-00\"),\n",
    "Seq(\"2020-10-01-00\", \"2020-11-01-00\"),\n",
    "Seq(\"2020-09-01-00\", \"2020-10-01-00\"),\n",
    "Seq(\"2020-08-01-00\", \"2020-09-01-00\")\n",
    ")\n",
    "\n",
    "//val objective = objectives(0)\n",
    "//val metrics = Seq(\"comments\")\n",
    "val path = \"dalids:///data_derived.ads_reporting_engagements_tracking_metrics_adengagementevent_daily\"\n",
    "for(month <- months){\n",
    "for(metric <- metrics){\n",
    "for(objective <- objectives){\n",
    "    val start = month(0)\n",
    "    val end = month(1)\n",
    "    print(start,end)\n",
    "    val filter = scala.collection.immutable.Map(DaliSpark.FILTER_EXP -> s\"datepartition >= '$start' and datepartition < '$end'\",DaliSpark.PROJECT_COLS -> s\"header.memberid,accounturn,$metric,datepartition,objectivetype\") // note I moved this back two years, tons of data.\n",
    "    val data = DaliSpark.createDataFrame(path, filter)//.withColumn(\"memberId\",$\"header.memberid\").drop($\"header\")\n",
    "    \n",
    "    val withObjectiveType = data.filter($\"objectivetype\" === objective).repartition()\n",
    "// you can count the number of empty entries and add these at the end???\n",
    "    val withMostRecentDateOfEngagement = DaliSpark.createDataFrame(\"/jobs/tscprep/test/Chad/Metrics/\" + metric).as[AccountMemberMostRecentEngagementDate].\n",
    "        withColumn(\"mostRecentEngagementDate\", substring($\"mostRecentEngagementDate\", 1, 10)).repartition($\"accounturn\") // 800 partitions\n",
    "    val withMostRecentEngagementDate = withObjectiveType.join(withMostRecentDateOfEngagement, Seq(\"accounturn\",\"memberid\"), \"left\"). // do not consider persons who never engage?  Maybe we can consider these as active members\n",
    "    withColumn(\"mostRecentEngagementDate\",when($\"mostRecentEngagementDate\"isNull, \"2021-08-01\").otherwise($\"mostRecentEngagementDate\"))\n",
    "    val asDate = withMostRecentEngagementDate.withColumn(\"timestamp\", substring($\"datepartition\", 1, 10)).\n",
    "    drop($\"datepartition\")\n",
    "    val withDateDifference = asDate.withColumn(\"difference\", datediff($\"mostRecentEngagementDate\", $\"timestamp\")).\n",
    "    drop($\"mostRecentEngagementDate\").\n",
    "    drop($\"timestamp\").\n",
    "    repartition(800, $\"accounturn\").\n",
    "    cache\n",
    "    //val historyCutoff = 365*2 // only grabbing one day worth of data anyways\n",
    "    //val filterByDateCutoff = withDateDifference.filter($\"difference\" < historyCutoff && $\"difference\" > 0)//.cache\n",
    "\n",
    "//    hdfsLogger.info(s\"Count of withDateDifference = ${withDateDifference.count}\")\n",
    "    \n",
    "    \n",
    "    val working = withDateDifference\n",
    "    val withBins = working.\n",
    "      withColumn(\"D0_1\", when($\"difference\" <= 1,1).otherwise(0)).\n",
    "      withColumn(\"D2_3\", when($\"difference\" >= 2 && $\"difference\" <= 3,1).otherwise(0)).\n",
    "      withColumn(\"D4_7\", when($\"difference\" >= 4 && $\"difference\" <= 7,1).otherwise(0)).\n",
    "      withColumn(\"D8_30\", when($\"difference\" >= 8 && $\"difference\" <= 30,1).otherwise(0)).\n",
    "      withColumn(\"D31_60\", when($\"difference\" >= 31 && $\"difference\" <= 60,1).otherwise(0)).\n",
    "      withColumn(\"D61_90\", when($\"difference\" >= 61 && $\"difference\" <= 90,1).otherwise(0)).\n",
    "      withColumn(\"D91_120\", when($\"difference\" >= 91 && $\"difference\" <= 120,1).otherwise(0)).\n",
    "      withColumn(\"D121_150\", when($\"difference\" >= 121 && $\"difference\" <= 150,1).otherwise(0)).\n",
    "      withColumn(\"D151_180\", when($\"difference\" >= 151 && $\"difference\" <= 180,1).otherwise(0)).\n",
    "      withColumn(\"D181_210\", when($\"difference\" >= 181 && $\"difference\" <= 210,1).otherwise(0)).\n",
    "      withColumn(\"D211_240\", when($\"difference\" >= 211 && $\"difference\" <= 240,1).otherwise(0)).\n",
    "      withColumn(\"D241_270\", when($\"difference\" >= 241 && $\"difference\" <= 270,1).otherwise(0)).\n",
    "      withColumn(\"D271_300\", when($\"difference\" >= 271 && $\"difference\" <= 300,1).otherwise(0)).\n",
    "      withColumn(\"D301_330\", when($\"difference\" >= 301 && $\"difference\" <= 330,1).otherwise(0)).\n",
    "      withColumn(\"D331_360\", when($\"difference\" >= 331 && $\"difference\" <= 360,1).otherwise(0)).\n",
    "    drop($\"difference\").\n",
    "    repartition(800, $\"accounturn\").\n",
    "    cache\n",
    "    \n",
    "//    hdfsLogger.info(s\"Count of withBins = ${withBins.count}\")\n",
    "    \n",
    "\n",
    "      val aggregate = withBins.groupBy($\"accounturn\",$\"memberid\",$\"objectivetype\").agg(\n",
    "        sum(col(metric)).alias(\"summation\"),\n",
    "        sum($\"D0_1\").alias(\"D0_1\"),\n",
    "        sum($\"D2_3\").alias(\"D2_3\"),\n",
    "        sum($\"D4_7\").alias(\"D4_7\"),\n",
    "        sum($\"D8_30\").alias(\"D8_30\"),\n",
    "        sum($\"D31_60\").alias(\"D31_60\"),\n",
    "        sum($\"D61_90\").alias(\"D61_90\"),\n",
    "        sum($\"D91_120\").alias(\"D91_120\"),\n",
    "        sum($\"D121_150\").alias(\"D121_150\"),\n",
    "        sum($\"D151_180\").alias(\"D151_180\"),\n",
    "        sum($\"D181_210\").alias(\"D181_210\"),\n",
    "        sum($\"D211_240\").alias(\"D211_240\"),\n",
    "        sum($\"D241_270\").alias(\"D241_270\"),\n",
    "        sum($\"D271_300\").alias(\"D271_300\"),\n",
    "        sum($\"D301_330\").alias(\"D301_330\"),\n",
    "        sum($\"D331_360\").alias(\"D331_360\")\n",
    "      ).\n",
    "    repartition().\n",
    "    cache\n",
    "\n",
    "//    hdfsLogger.info(s\"Count of aggregate = ${aggregate.count}\")\n",
    "    \n",
    "    val brand_awareness = aggregate.//.filter($\"objectivetype\" === \"BRAND_AWARENESS\").\n",
    "    drop($\"objectivetype\").\n",
    "    withColumn(\"awareness_summation\", when($\"summation\" > 0, 1).otherwise(0)).\n",
    "    drop($\"summation\").\n",
    "    withColumn(\"awareness_0_1\",$\"D0_1\").\n",
    "    drop($\"D0_1\").\n",
    "    withColumn(\"awareness_2_3\",$\"D2_3\").\n",
    "    drop($\"D2_3\").\n",
    "    withColumn(\"awareness_4_7\",$\"D4_7\").\n",
    "    drop($\"D4_7\").\n",
    "    withColumn(\"awareness_8_30\",$\"D8_30\").\n",
    "    drop($\"D8_30\").\n",
    "    withColumn(\"awareness_31_60\",$\"D31_60\").\n",
    "    drop($\"D31_60\").\n",
    "    withColumn(\"awareness_61_90\",$\"D61_90\").\n",
    "    drop($\"D61_90\").\n",
    "    withColumn(\"awareness_91_120\",$\"D91_120\").\n",
    "    drop($\"D91_120\").\n",
    "    withColumn(\"awareness_121_150\",$\"D121_150\").\n",
    "    drop($\"D121_150\").\n",
    "    withColumn(\"awareness_151_180\",$\"D151_180\").\n",
    "    drop($\"D151_180\").\n",
    "    withColumn(\"awareness_181_210\",$\"D181_210\").\n",
    "    drop($\"D181_210\").\n",
    "    withColumn(\"awareness_211_240\",$\"D211_240\").\n",
    "    drop($\"D211_240\").\n",
    "    withColumn(\"awareness_241_270\",$\"D241_270\").\n",
    "    drop($\"D241_270\").\n",
    "    withColumn(\"awareness_271_300\",$\"D271_300\").\n",
    "    drop($\"D271_300\").\n",
    "    withColumn(\"awareness_301_330\",$\"D301_330\").\n",
    "    drop($\"D301_330\").\n",
    "    withColumn(\"awareness_331_360\",$\"D331_360\").\n",
    "    drop($\"D331_360\").\n",
    "    cache\n",
    "    \n",
    "//    hdfsLogger.info(s\"Count of brand_awareness = ${brand_awareness.count}\")\n",
    "    \n",
    "      //val joined = brand_awareness.join(website_visit, Seq(\"accounturn\",\"memberid\"), \"inner\").repartition(800)//.\n",
    "      val writePath = \"/jobs/tscprep/test/Chad/\" + metric + \"/\" + objective + \"/\" + start\n",
    "      DaliSpark.writeDataFrame(brand_awareness, writePath, scala.collection.immutable.Map(DaliSpark.OVERWRITE_MODE -> \"ALL\", DaliSpark.OUTPUT_PARALLELISM -> \"10\"))\n",
    "    }\n",
    "}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-02T13:16:50.259204Z",
     "start_time": "2021-08-02T13:16:44.212785Z"
    }
   },
   "outputs": [],
   "source": [
    "data.rdd.getNumPartitions // data has a massive number of partitions to work from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-01T02:06:25.453953Z",
     "start_time": "2021-08-01T02:06:09.332723Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.DaliSpark\n",
    "val result = DaliSpark.createDataFrame(\"/jobs/tscprep/test/Chad/shares/*\")\n",
    "result.select(sum(\"visit_summation\")).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-12T15:23:09.768978Z",
     "start_time": "2021-08-12T14:58:55.092612Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.DaliSpark\n",
    "import collection.JavaConverters._\n",
    "import spark.implicits._\n",
    "import collection.mutable._\n",
    "import scala.util.control._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "// goal is to combine the account/member data for all months and store these elsewhere.\n",
    "// I also need to combine brand_awareness and website_visit counts into the same dataset\n",
    "// the default is awareness\n",
    "\n",
    "// step 1 is to combine the different months\n",
    "// step 2 is to combine brand awareness and website visits\n",
    "\n",
    "def aggregateMonths(df: DataFrame): DataFrame = {\n",
    "          df.groupBy($\"accounturn\",$\"memberid\").agg(\n",
    "        sum(\"awareness_summation\").alias(\"awareness_summation\"),\n",
    "        sum($\"awareness_0_1\").alias(\"awareness_0_1\"),\n",
    "        sum($\"awareness_2_3\").alias(\"awareness_2_3\"),\n",
    "        sum($\"awareness_4_7\").alias(\"awareness_4_7\"),\n",
    "        sum($\"awareness_8_30\").alias(\"awareness_8_30\"),\n",
    "        sum($\"awareness_31_60\").alias(\"awareness_31_60\"),\n",
    "        sum($\"awareness_61_90\").alias(\"awareness_61_90\"),\n",
    "        sum($\"awareness_91_120\").alias(\"awareness_91_120\"),\n",
    "        sum($\"awareness_121_150\").alias(\"awareness_121_150\"),\n",
    "        sum($\"awareness_151_180\").alias(\"awareness_151_180\"),\n",
    "        sum($\"awareness_181_210\").alias(\"awareness_181_210\"),\n",
    "        sum($\"awareness_211_240\").alias(\"awareness_211_240\"),\n",
    "        sum($\"awareness_241_270\").alias(\"awareness_241_270\"),\n",
    "        sum($\"awareness_271_300\").alias(\"awareness_271_300\"),\n",
    "        sum($\"awareness_301_330\").alias(\"awareness_301_330\"),\n",
    "        sum($\"awareness_331_360\").alias(\"awareness_331_360\")\n",
    "      ).\n",
    "    repartition().\n",
    "    cache\n",
    "}\n",
    "\n",
    "val metrics = Seq(\n",
    "\"landingpageclicks\",\n",
    "\"totalengagements\",\n",
    "\"otherengagements\",\n",
    "\"likes\",\n",
    "\"commentlikes\",\n",
    "\"comments\",\n",
    "\"shares\",\n",
    "\"follows\"\n",
    ")\n",
    "//val metric = \"landingpageclicks\"\n",
    "for(metric <- metrics){\n",
    "    val basePath = \"/jobs/tscprep/test/Chad/\" + metric\n",
    "    val websiteVisits = DaliSpark.createDataFrame(basePath + \"/\" + \"WEBSITE_VISIT\" + \"/*\")\n",
    "\n",
    "    val websiteVisitsAggregate = aggregateMonths(websiteVisits).\n",
    "        withColumnRenamed(\"awareness_summation\", \"visit_summation\").\n",
    "        withColumnRenamed(\"awareness_0_1\",\"visit_0_1\").\n",
    "        withColumnRenamed(\"awareness_2_3\",\"visit_2_3\").\n",
    "        withColumnRenamed(\"awareness_4_7\",\"visit_4_7\").\n",
    "        withColumnRenamed(\"awareness_8_30\",\"visit_8_30\").\n",
    "        withColumnRenamed(\"awareness_31_60\",\"visit_31_60\").\n",
    "        withColumnRenamed(\"awareness_61_90\",\"visit_61_90\").\n",
    "        withColumnRenamed(\"awareness_91_120\",\"visit_91_120\").\n",
    "        withColumnRenamed(\"awareness_121_150\",\"visit_121_150\").\n",
    "        withColumnRenamed(\"awareness_151_180\",\"visit_151_180\").\n",
    "        withColumnRenamed(\"awareness_181_210\",\"visit_181_210\").\n",
    "        withColumnRenamed(\"awareness_211_240\",\"visit_211_240\").\n",
    "        withColumnRenamed(\"awareness_241_270\",\"visit_241_270\").\n",
    "        withColumnRenamed(\"awareness_271_300\",\"visit_271_300\").\n",
    "        withColumnRenamed(\"awareness_301_330\",\"visit_301_330\").\n",
    "        withColumnRenamed(\"awareness_331_360\",\"visit_331_360\")\n",
    "\n",
    "    val visitWritePath = s\"/jobs/tscprep/test/Chad/Combined/$metric/visit\"\n",
    "    DaliSpark.writeDataFrame(websiteVisitsAggregate, visitWritePath, scala.collection.immutable.Map(DaliSpark.OVERWRITE_MODE -> \"ALL\", DaliSpark.OUTPUT_PARALLELISM -> \"50\"))\n",
    "\n",
    "    val brandAwareness = DaliSpark.createDataFrame(basePath + \"/\" + \"BRAND_AWARENESS\" + \"/*\")\n",
    "    val brandAwarenessAggregate = aggregateMonths(brandAwareness)\n",
    "    val awarenessWritePath = s\"/jobs/tscprep/test/Chad/Combined/$metric/awareness\"\n",
    "    DaliSpark.writeDataFrame(brandAwarenessAggregate, awarenessWritePath, scala.collection.immutable.Map(DaliSpark.OVERWRITE_MODE -> \"ALL\", DaliSpark.OUTPUT_PARALLELISM -> \"50\"))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-13T07:31:47.519996Z",
     "start_time": "2021-08-13T07:31:43.475053Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.DaliSpark\n",
    "import collection.JavaConverters._\n",
    "import spark.implicits._\n",
    "import collection.mutable._\n",
    "import scala.util.control._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "val metrics = Seq(\n",
    "\"landingpageclicks\",\n",
    "\"totalengagements\",\n",
    "\"otherengagements\",\n",
    "\"likes\",\n",
    "\"commentlikes\",\n",
    "\"comments\",\n",
    "\"shares\",\n",
    "\"follows\"\n",
    ")\n",
    "for(metric <- metrics){\n",
    "    val awareness = DaliSpark.createDataFrame(s\"/jobs/tscprep/test/Chad/Combined/$metric/awareness\").repartition(800)\n",
    "    val visit = DaliSpark.createDataFrame(s\"/jobs/tscprep/test/Chad/Combined/$metric/visit\").repartition(800)\n",
    "    val joined: DataFrame = awareness.join(visit, Seq(\"accounturn\",\"memberid\"), \"inner\").\n",
    "    drop($\"memberid\").\n",
    "    drop($\"accounturn\")\n",
    "    val joinedWritePath = s\"/jobs/tscprep/test/Chad/Joined/$metric\"\n",
    "    //if(DaliSpark.createDataFrame(joinedWritePath).rdd.isEmpty){\n",
    "//        println(s\"$metric\")\n",
    "//        DaliSpark.writeDataFrame(joined, joinedWritePath, scala.collection.immutable.Map(DaliSpark.OVERWRITE_MODE -> \"ALL\", DaliSpark.OUTPUT_PARALLELISM -> \"5\"))\n",
    "        joined.\n",
    "    repartition(20).\n",
    "    write.mode(\"overwrite\").option(\"header\", \"true\").option(\"sep\", \"|\").csv(joinedWritePath)\n",
    "    //}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-03T23:13:22.683214Z",
     "start_time": "2021-08-03T23:13:20.642141Z"
    }
   },
   "outputs": [],
   "source": [
    "print(awareness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-13T07:12:37.014018Z",
     "start_time": "2021-08-13T07:10:38.486049Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.DaliSpark\n",
    "import collection.JavaConverters._\n",
    "import spark.implicits._\n",
    "import collection.mutable._\n",
    "import scala.util.control._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Dataset\n",
    "\n",
    "// purpose is to aggregate files\n",
    "val metrics = Seq(\n",
    "\"landingpageclicks\",\n",
    "\"totalengagements\",\n",
    "\"otherengagements\",\n",
    "\"likes\",\n",
    "\"commentlikes\",\n",
    "\"comments\",\n",
    "\"shares\",\n",
    "\"follows\"\n",
    ")\n",
    "val objectives = Seq(\n",
    "    \"BRAND_AWARENESS\",\n",
    "    \"WEBSITE_VISIT\"\n",
    ")\n",
    "\n",
    "val months = Seq(\n",
    "Seq(\"2021-07-01-00\", \"2021-08-01-00\"),\n",
    "Seq(\"2021-06-01-00\", \"2021-07-01-00\"),\n",
    "Seq(\"2021-05-01-00\", \"2021-06-01-00\"),\n",
    "Seq(\"2021-04-01-00\", \"2021-05-01-00\"),\n",
    "Seq(\"2021-03-01-00\", \"2021-04-01-00\"),    \n",
    "Seq(\"2021-02-01-00\", \"2021-03-01-00\"),\n",
    "Seq(\"2021-01-01-00\", \"2021-02-01-00\"),\n",
    "Seq(\"2020-12-01-00\", \"2021-01-01-00\"),\n",
    "Seq(\"2020-11-01-00\", \"2020-12-01-00\"),\n",
    "Seq(\"2020-10-01-00\", \"2020-11-01-00\"),\n",
    "Seq(\"2020-09-01-00\", \"2020-10-01-00\"),\n",
    "Seq(\"2020-08-01-00\", \"2020-09-01-00\")\n",
    ")\n",
    "\n",
    "//val objective = objectives(0)\n",
    "//val metrics = Seq(\"comments\")\n",
    "import scala.util.control._\n",
    "\n",
    "// create a Breaks object as follows\n",
    "val loop = new Breaks;\n",
    "\n",
    "// Keep the loop inside breakable as follows\n",
    "loop.breakable{\n",
    "for(metric <- metrics){  \n",
    "    val loadPath = s\"/jobs/tscprep/test/Chad/Joined/$metric\"\n",
    "    //val data = DaliSpark.createDataFrame(loadPath)\n",
    "    val data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\",\"|\").load(loadPath)\n",
    "    val tempSaveLocation = \"/jobs/tscprep/test/Chad/temp/\" + metric\n",
    "    DaliSpark.writeDataFrame(data.repartition(3), tempSaveLocation, scala.collection.immutable.Map(DaliSpark.OVERWRITE_MODE -> \"ALL\", DaliSpark.OUTPUT_PARALLELISM -> \"3\"))\n",
    "    //val reload = DaliSpark.createDataFrame(tempSaveLocation)\n",
    "    val reload = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\",\"|\").load(tempSaveLocation)\n",
    "    println(s\"${reload.count} and ${data.count}\")\n",
    "    reload.repartition(5).\n",
    "    drop($\"accounturn\").\n",
    "    drop($\"memberid\").\n",
    "    write.mode(\"overwrite\").option(\"header\", \"true\").option(\"sep\", \"|\").csv(loadPath)\n",
    "    //DaliSpark.writeDataFrameAsCsv(reload.repartition(1), loadPath, scala.collection.immutable.Map(DaliSpark.OVERWRITE_MODE -> \"ALL\", DaliSpark.OUTPUT_PARALLELISM -> \"1\"))\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-13T07:15:05.869972Z",
     "start_time": "2021-08-13T07:14:59.806275Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution_time": {
     "end_time": "2021-08-05T17:26:06.850493Z",
     "start_time": "2021-08-05T17:26:04.807502Z"
    }
   },
   "outputs": [],
   "source": [
    "//spark.read.format(\"csv\").options(header=\"true\").load(loadPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "darwin": {
   "resource_id": 114696
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}